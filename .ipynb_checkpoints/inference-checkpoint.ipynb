{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8099fac-98cd-4907-ad9e-1530348ef2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "with open(\"synth_data_v2.json\",\"r\") as f:\n",
    "    corpse = json.loads(f.read())\n",
    "trainset = []\n",
    "for i in corpse:\n",
    "    try:\n",
    "        trainset.append(json.loads(i)['text'])\n",
    "    except:\n",
    "        continue\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "\n",
    "# model\n",
    "class FASG_Model(nn.Module):\n",
    "    def __init__(self,device=\"cuda:0\"):\n",
    "        super(FASG_Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bertModel = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "        self.linear = torch.nn.Linear(768, 768).to(self.device)\n",
    "        self.llm = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=self.device)\n",
    "        self.sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=\"gpt2-small-res-jb-feature-splitting\",  \n",
    "            sae_id=\"blocks.8.hook_resid_pre_768\", \n",
    "            device=self.device,\n",
    "        )\n",
    "        self.top_p = 0.95\n",
    "        self.temperature = 0.5\n",
    "        \n",
    "    def bert_tokenize(self,text):\n",
    "        return self.bertTokenizer(text,padding=True,truncation=True,return_tensors=\"pt\").to(device = self.device)\n",
    "        \n",
    "    def forward(self, encoded_input,prompt,return_type = \"str\"):\n",
    "        steering_vector = self.bertModel(**encoded_input).pooler_output\n",
    "\n",
    "        def steering_features(value, hook,steering_vector = steering_vector):\n",
    "            encoded_activation = self.sae.encode(value)\n",
    "            steered_vector = steering_vector.unsqueeze(1)*encoded_activation \n",
    "            decoded_vector = self.sae.decode(steered_vector)\n",
    "            return decoded_vector\n",
    "    \n",
    "        fwd_hooks=[(\n",
    "            'blocks.8.hook_resid_pre', \n",
    "            steering_features\n",
    "        )]\n",
    "\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        with self.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "            steered_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return steered_tokens\n",
    "\n",
    "    def llm_generate(self,prompt,return_type = \"str\"):\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        generated_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return generated_tokens\n",
    "        \n",
    "\n",
    "    def freeze_llm_and_sae(self):\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.sae.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# loss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_util\n",
    "\n",
    "class FASG_Loss(nn.Module):\n",
    "    def __init__(self,device=\"cuda:0\"):\n",
    "        super(FASG_Loss, self).__init__()\n",
    "        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, reference_text,steered_text,baseline_text,temp= 1):\n",
    "        tokenized_inputs = self.sentence_transformer.tokenize(\n",
    "            [reference_text, steered_text, baseline_text])\n",
    "        # move all tensors to gpu\n",
    "        for key in tokenized_inputs.keys():\n",
    "            tokenized_inputs[key] = tokenized_inputs[key].to(self.device)\n",
    "        embeddings = self.sentence_transformer(tokenized_inputs)['sentence_embedding']\n",
    "        embeddings = embeddings * temp\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        sim_positive = 1-st_util.cos_sim(embeddings[0], embeddings[1])\n",
    "        sim_negative = 1-st_util.cos_sim(embeddings[0], embeddings[2])\n",
    "\n",
    "        # Compute softmax triplet loss\n",
    "        margin = 0.2\n",
    "        triplet_loss = torch.max(sim_positive-sim_negative+margin,0)[0] # torch mx returns (max, max_indices)\n",
    "        loss = triplet_loss + sim_positive *0.1 # still preserve a little \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "823fdac8-7955-4498-b064-39523ded806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594907/344390977.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_ckpts/fasg_model_epoch4.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FASG_Model()\n",
    "model.load_state_dict(torch.load(\"model_ckpts/fasg_model_epoch4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6124cee-0528-473d-b1d8-1e9ac0439963",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synth_data_testset.txt\",\"r\") as f:\n",
    "    testset = [json.loads(i) for i in f.read().split(\"\\n\\n\") if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bcab184-cd23-48df-920a-621326bd55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsteered: \n",
      "\n",
      "I have a little story to tell about my life. I am a very young man. I am a very young man. I am a very young man. I am a very young man. I am a very young man. I am a very young man. I am a very young man. I am a very young man. I am\n",
      "Steered: \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FASG_Model' object has no attribute 'tempertature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mllm_generate(prompt))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteered: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 60\u001b[0m, in \u001b[0;36mFASG_Model.forward\u001b[0;34m(self, encoded_input, prompt, return_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m tokenized_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mto_tokens(prompt)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks\u001b[38;5;241m=\u001b[39mfwd_hooks):\n\u001b[1;32m     57\u001b[0m     steered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     58\u001b[0m     tokenized_prompt,\n\u001b[1;32m     59\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m---> 60\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtempertature\u001b[49m,\n\u001b[1;32m     61\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_p,\n\u001b[1;32m     62\u001b[0m     stop_at_eos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m     return_type \u001b[38;5;241m=\u001b[39m return_type,\n\u001b[1;32m     64\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m steered_tokens\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FASG_Model' object has no attribute 'tempertature'"
     ]
    }
   ],
   "source": [
    "reference_text = testset[3]['text']\n",
    "prompt = \" \".join(reference_text.split(\" \")[:5])\n",
    "\n",
    "print(\"Unsteered: \\n\")\n",
    "print(model.llm_generate(prompt))\n",
    "print(\"Steered: \\n\")\n",
    "print(model(model.bert_tokenize(reference_text),prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431967-d513-4c38-a122-00a5e8ed282d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Small Data",
   "language": "python",
   "name": "small-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
