{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "823272db-0fcc-4e5e-9373-40dc4778f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "with open(\"synth_data_v2.json\",\"r\") as f:\n",
    "    corpse = json.loads(f.read())\n",
    "trainset = []\n",
    "for i in corpse:\n",
    "    try:\n",
    "        trainset.append(json.loads(i)['text'])\n",
    "    except:\n",
    "        continue\n",
    "print(f\"Train set size: {len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d42aab9a-50b6-4990-904c-6f981fae43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class FASG_Model(nn.Module):\n",
    "    def __init__(self,device=\"cuda:0\"):\n",
    "        super(FASG_Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bertModel = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "        self.linear = torch.nn.Linear(768, 768).to(self.device)\n",
    "        self.llm = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=self.device)\n",
    "        self.sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=\"gpt2-small-res-jb-feature-splitting\",  \n",
    "            sae_id=\"blocks.8.hook_resid_pre_768\", \n",
    "            device=self.device,\n",
    "        )\n",
    "        self.top_p = 0.95\n",
    "        self.temperature = 0.5\n",
    "        \n",
    "    def bert_tokenize(self,text):\n",
    "        return self.bertTokenizer(text,padding=True,truncation=True,return_tensors=\"pt\").to(device = self.device)\n",
    "        \n",
    "    def forward(self, encoded_input,prompt,return_type = \"str\"):\n",
    "        steering_vector = self.bertModel(**encoded_input).pooler_output\n",
    "\n",
    "        def steering_features(value, hook,steering_vector = steering_vector,steering_weights = 4):\n",
    "            #encoded_activation = self.sae.encode(value)\n",
    "            #steered_vector = steering_vector.unsqueeze(1)*encoded_activation \n",
    "            delta_activation = self.sae.decode(steering_vector)\n",
    "\n",
    "            steered_activation = delta_activation.unsqueeze(1)*steering_weights + value\n",
    "            \n",
    "            return steered_activation\n",
    "    \n",
    "        fwd_hooks=[(\n",
    "            'blocks.8.hook_resid_pre', \n",
    "            steering_features\n",
    "        )]\n",
    "\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        with self.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "            steered_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return steered_tokens\n",
    "\n",
    "    def llm_generate(self,prompt,return_type = \"str\"):\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        generated_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return generated_tokens\n",
    "        \n",
    "\n",
    "    def freeze_llm_and_sae(self):\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.sae.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6159bbbf-9a5c-4783-a11e-f48d0777e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yeah, but most folks think'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FASG_Model()\n",
    "sample = trainset[5]\n",
    "\n",
    "reference_text = sample\n",
    "prompt = \" \".join(sample.split(\" \")[:5])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04f4cfea-053b-48a4-8206-8eccccc67545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yeah, but most folks think the next big thing is the next big thing.\\n\\nThere are some serious, serious issues with the current political climate.\\n\\nThe first is that the GOP is basically trying to make it impossible to pass a healthcare bill, which is a pretty big deal. The GOP is trying to make it impossible to pass a'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llm_generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "483f8f61-c140-44ea-99f8-dd9205102c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah, but most folks think it's a \"natural\" thing to do. It's not.\n",
      "\n",
      "I'm not saying that you should be \"natural\" to do something. I'm just saying that it's not a natural thing to do, and it's not a natural thing to do if you're not doing it.\n",
      "\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "def steering_features(value, hook,steering_vector = steering_vector):\n",
    "    out = value + 0.1*steering_vector.unsqueeze(1)\n",
    "    return out\n",
    "\n",
    "fwd_hooks=[(\n",
    "    'blocks.8.hook_resid_pre', \n",
    "    steering_features\n",
    ")]\n",
    "\n",
    "tokenized_prompt = model.llm.to_tokens(prompt)\n",
    "with model.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "    steered = model.llm.generate(\n",
    "    tokenized_prompt,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    stop_at_eos = True,\n",
    "    return_type = \"str\",\n",
    "    verbose = False\n",
    ")\n",
    "print(steered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f308fe7b-0adc-4ca6-9e37-f38a4ff1f8ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HookedTransformer.forward() got an unexpected keyword argument 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: HookedTransformer.forward() got an unexpected keyword argument 'loss'"
     ]
    }
   ],
   "source": [
    "model.llm.forward(prompt, loss=True, return_type='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611ce00-ca21-4af6-986b-c239f36b0a8a",
   "metadata": {},
   "source": [
    "# t test|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c668c0e-db87-46d2-a13c-4035e36d79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steered_output(model, prompt, steering_vector):\n",
    "    steering_vector = steering_vector\n",
    "    \n",
    "    def steering_features(value, hook,steering_vector = steering_vector):\n",
    "        out = value + 0.1*steering_vector.unsqueeze(1)\n",
    "        return out\n",
    "    \n",
    "    fwd_hooks=[(\n",
    "        'blocks.8.hook_resid_pre', \n",
    "        steering_features\n",
    "    )]\n",
    "    \n",
    "    tokenized_prompt = model.llm.to_tokens(prompt)\n",
    "    with model.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "        steered = model.llm.generate(\n",
    "        tokenized_prompt,\n",
    "        max_new_tokens=64,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        stop_at_eos = True,\n",
    "        return_type = \"str\",\n",
    "        verbose = False\n",
    "    )\n",
    "    return steered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62f04018-4d92-4477-8713-d83186d07893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_util\n",
    "\n",
    "\n",
    "def get_cos_sim(st_model,text1,text2):\n",
    "    tokenized_inputs = st_model.tokenize(\n",
    "                [text1,text2])\n",
    "    for key in tokenized_inputs.keys():\n",
    "        tokenized_inputs[key] = tokenized_inputs[key].to(\"cuda:0\")\n",
    "    embeddings = st_model(tokenized_inputs)['sentence_embedding']\n",
    "    sim = st_util.cos_sim(embeddings[0], embeddings[1])\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f905c99-6c6a-40d8-bc1c-7868cd16ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_dict = {\"rouge1\":[],\n",
    "             \"rouge2\":[],\n",
    "             \"rougeL\":[]}\n",
    "cos_sim = []\n",
    "\n",
    "# top 10 features\n",
    "top_features = [698, 734, 709, 719,  87, 112, 675, 679, 187, 445]\n",
    "num_steered_features = 5\n",
    "steering_strength = 1\n",
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for sample in tqdm(trainset[:100]):\n",
    "    prompt = \" \".join(sample.split(\" \")[:5])\n",
    "\n",
    "    unsteered_output = model.llm_generate(prompt)\n",
    "    steering_vec = np.zeros(768,dtype = np.float32)\n",
    "    steering_vec[top_features[:num_steered_features]] = steering_strength\n",
    "    steering_vec = torch.tensor(steering_vec).unsqueeze(0).to(\"cuda:0\")\n",
    "\n",
    "    delta_activation = model.sae.decode(steering_vec)\n",
    "    steered_output = get_steered_output(model, prompt,delta_activation)\n",
    "\n",
    "    # rouge\n",
    "    rouge_scores = scorer.score(unsteered_output, steered_output)\n",
    "    for key in rouge_dict.keys():\n",
    "        rouge_dict[key].append(rouge_scores[key])\n",
    "    \n",
    "\n",
    "    # cos sim\n",
    "    cos_sim.append(get_cos_sim(st_model,unsteered_output,steered_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Small Data",
   "language": "python",
   "name": "small-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
