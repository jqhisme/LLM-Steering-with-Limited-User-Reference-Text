{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef201a10-1c48-4c18-aa25-b1839579b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE, HookedSAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4281ed03-2b2d-43f6-8d45-2db210ca95eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb-feature-splitting\",  # <- Release name\n",
    "    sae_id=\"blocks.8.hook_resid_pre_768\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6085e15a-a9a5-4ac3-9e76-d32f1d15f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# Input sentence\n",
    "input_sentence = \"Who are you? I am\"\n",
    "inputs = model.tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get model outputs\n",
    "with torch.no_grad():\n",
    "    logits, activations = model.run_with_cache(input_sentence)\n",
    "\n",
    "print(sae.encode_standard(activations['blocks.8.hook_resid_pre']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09bb2283-1de9-44ac-ac8f-e292066f46e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.decode(sae.encode_standard(activations['blocks.8.hook_resid_pre'])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a174ad64-45ef-460b-94e1-17f1dff39aa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FASG_Model' object has no attribute 'hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m fwd_hooks\u001b[38;5;241m=\u001b[39m[(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.8.hook_resid_pre\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     12\u001b[0m         steering_features\n\u001b[1;32m     13\u001b[0m         )]\n\u001b[1;32m     14\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am a person who\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m(fwd_hooks\u001b[38;5;241m=\u001b[39mfwd_hooks):\n\u001b[1;32m     17\u001b[0m     steered_text \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     18\u001b[0m         prompt,\n\u001b[1;32m     19\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(steered_text)\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FASG_Model' object has no attribute 'hooks'"
     ]
    }
   ],
   "source": [
    "def steering_features(value, hook):\n",
    "    # BERT 生成的768vector*activation vector\n",
    "    \n",
    "    #encoded_activation = sae.encode(value)\n",
    "    #steered_vector = steering_vector*encoded_activation \n",
    "    #value = sae.decode(steered_vector)\n",
    "    return value\n",
    "    \n",
    "\n",
    "fwd_hooks=[(\n",
    "        'blocks.8.hook_resid_pre', \n",
    "        steering_features\n",
    "        )]\n",
    "prompt = \"I am a person who\"\n",
    "\n",
    "with model.hooks(fwd_hooks=fwd_hooks):\n",
    "    steered_text = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        stop_at_eos = True,\n",
    "        \n",
    "    )\n",
    "\n",
    "print(steered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c3bdfa0-5217-44b3-a74b-aaed034bcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pseudo code\n",
    "\n",
    "# Step 1: Use BERT to predict the 768 vector -> steering vector\n",
    "\n",
    "# Our own model\n",
    "class BertWithDense(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", output_dim=768):\n",
    "        pass\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pass\n",
    "\n",
    "bert_model = BertWithDense()\n",
    "#bert_model(tokenizer(x)) <--768 vector\n",
    "\n",
    "# Step 2: Generate Text with LLM\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "# model\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# sae\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb-feature-splitting\",  # <- Release name\n",
    "    sae_id=\"blocks.8.hook_resid_pre_768\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def steering_features(value, hook):\n",
    "    # BERT 生成的768vector*activation vector\n",
    "    \n",
    "    #encoded_activation = sae.encode(value)\n",
    "    #steered_vector = steering_vector*encoded_activation \n",
    "    #value = sae.decode(steered_vector)\n",
    "    return value \n",
    "\n",
    "fwd_hooks=[(\n",
    "        'blocks.8.hook_resid_pre', \n",
    "        steering_features\n",
    "        )]\n",
    "\n",
    "\n",
    "prompt = x[:10]\n",
    "\n",
    "with model.hooks(fwd_hooks=fwd_hooks):\n",
    "    steered_text = model.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        stop_at_eos = True,\n",
    "        \n",
    "    )\n",
    "\n",
    "# Step 3 -> calculate loss and gradient descent\n",
    "# loss =  cosine_similarity(x,steered_text)\n",
    "# with bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9379aae9-b6ab-449a-bb31-9c196a12b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class FASG_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FASG_Model, self).__init__()\n",
    "\n",
    "        self.device = \"cuda:1\"\n",
    "        self.bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bertModel = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "        self.linear = torch.nn.Linear(768, 768).to(self.device)\n",
    "        self.llm = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=self.device)\n",
    "        self.sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=\"gpt2-small-res-jb-feature-splitting\",  \n",
    "            sae_id=\"blocks.8.hook_resid_pre_768\", \n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        encoded_input = tokenizer(text, return_tensors='pt',padding=True, truncation=True)\n",
    "        steering_vector = self.bertModel(**encoded_input).pooler_output\n",
    "\n",
    "        def steering_features(value, hook):\n",
    "            encoded_activation = self.sae.encode(value)\n",
    "            print(encoded_activation.shape)\n",
    "            print(steering_vector.shape)\n",
    "            steered_vector = steering_vector*encoded_activation \n",
    "            decoded_vector = sae.decode(steered_vector)\n",
    "            return decoded_vector\n",
    "    \n",
    "        fwd_hooks=[(\n",
    "            'blocks.8.hook_resid_pre', \n",
    "            steering_features\n",
    "        )]\n",
    "\n",
    "        prompt = \" \".join(text.split(\" \")[:7])\n",
    "        \n",
    "        with self.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "            steered_tokens = self.llm.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos = True,\n",
    "            return_type = \"tensor\"\n",
    "        )\n",
    "        return steered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7612518-8f2f-4f2a-a2e7-c10f7915c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-qj351/.conda/envs/small-data/lib/python3.11/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is a sample text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m FASG_Model()\n\u001b[0;32m----> 3\u001b[0m result\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/small-data/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mFASG_Model.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 26\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbertModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msteering_features\u001b[39m(value, hook):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "sample_text = \"this is a sample text\"\n",
    "model = FASG_Model()\n",
    "result= model(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ad2f614b-e817-489f-938a-681f8afe74ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'devices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'devices'"
     ]
    }
   ],
   "source": [
    "torch.cuda.devices[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Small Data",
   "language": "python",
   "name": "small-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
