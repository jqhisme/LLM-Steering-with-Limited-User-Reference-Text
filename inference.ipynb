{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8099fac-98cd-4907-ad9e-1530348ef2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 20746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "with open(\"synth_data_v2.json\",\"r\") as f:\n",
    "    corpse = json.loads(f.read())\n",
    "trainset = []\n",
    "for i in corpse:\n",
    "    try:\n",
    "        trainset.append(json.loads(i)['text'])\n",
    "    except:\n",
    "        continue\n",
    "print(f\"Train set size: {len(trainset)}\")\n",
    "\n",
    "# model\n",
    "class FASG_Model(nn.Module):\n",
    "    def __init__(self,device=\"cuda:0\"):\n",
    "        super(FASG_Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bertModel = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "        self.linear = torch.nn.Linear(768, 768).to(self.device)\n",
    "        self.llm = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=self.device)\n",
    "        self.sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=\"gpt2-small-res-jb-feature-splitting\",  \n",
    "            sae_id=\"blocks.8.hook_resid_pre_768\", \n",
    "            device=self.device,\n",
    "        )\n",
    "        self.top_p = 0.95\n",
    "        self.temperature = 0.5\n",
    "        \n",
    "    def bert_tokenize(self,text):\n",
    "        return self.bertTokenizer(text,padding=True,truncation=True,return_tensors=\"pt\").to(device = self.device)\n",
    "        \n",
    "    def forward(self, encoded_input,prompt,return_type = \"str\"):\n",
    "        steering_vector = self.bertModel(**encoded_input).pooler_output\n",
    "\n",
    "        def steering_features(value, hook,steering_vector = steering_vector):\n",
    "            encoded_activation = self.sae.encode(value)\n",
    "            steered_vector = steering_vector.unsqueeze(1)*encoded_activation \n",
    "            decoded_vector = self.sae.decode(steered_vector)\n",
    "            return decoded_vector\n",
    "    \n",
    "        fwd_hooks=[(\n",
    "            'blocks.8.hook_resid_pre', \n",
    "            steering_features\n",
    "        )]\n",
    "\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        with self.llm.hooks(fwd_hooks=fwd_hooks):\n",
    "            steered_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return steered_tokens\n",
    "\n",
    "    def llm_generate(self,prompt,return_type = \"str\"):\n",
    "        tokenized_prompt = self.llm.to_tokens(prompt)\n",
    "        generated_tokens = self.llm.generate(\n",
    "            tokenized_prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            stop_at_eos = True,\n",
    "            return_type = return_type,\n",
    "            verbose = False\n",
    "        )\n",
    "        return generated_tokens\n",
    "        \n",
    "\n",
    "    def freeze_llm_and_sae(self):\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.sae.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# loss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_util\n",
    "\n",
    "class FASG_Loss(nn.Module):\n",
    "    def __init__(self,device=\"cuda:0\"):\n",
    "        super(FASG_Loss, self).__init__()\n",
    "        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, reference_text,steered_text,baseline_text,temp= 1):\n",
    "        tokenized_inputs = self.sentence_transformer.tokenize(\n",
    "            [reference_text, steered_text, baseline_text])\n",
    "        # move all tensors to gpu\n",
    "        for key in tokenized_inputs.keys():\n",
    "            tokenized_inputs[key] = tokenized_inputs[key].to(self.device)\n",
    "        embeddings = self.sentence_transformer(tokenized_inputs)['sentence_embedding']\n",
    "        embeddings = embeddings * temp\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        sim_positive = 1-st_util.cos_sim(embeddings[0], embeddings[1])\n",
    "        sim_negative = 1-st_util.cos_sim(embeddings[0], embeddings[2])\n",
    "\n",
    "        # Compute softmax triplet loss\n",
    "        margin = 0.2\n",
    "        triplet_loss = torch.max(sim_positive-sim_negative+margin,0)[0] # torch mx returns (max, max_indices)\n",
    "        loss = triplet_loss + sim_positive *0.1 # still preserve a little \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "823fdac8-7955-4498-b064-39523ded806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594907/344390977.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_ckpts/fasg_model_epoch4.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FASG_Model()\n",
    "model.load_state_dict(torch.load(\"model_ckpts/fasg_model_epoch4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6124cee-0528-473d-b1d8-1e9ac0439963",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synth_data_testset.txt\",\"r\") as f:\n",
    "    testset = [json.loads(i) for i in f.read().split(\"\\n\\n\") if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bcab184-cd23-48df-920a-621326bd55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsteered: \n",
      "\n",
      "I have a little story for you: I was a child when I was young and my parents didn't have any money to pay my bills. I was a little bit homeless, and I didn't have a home. I was homeless when I was 12, but I was able to get by, and I was able to buy food, clothes\n",
      "Steered: \n",
      "\n",
      "I have a little story this the as well, to to. to to to. in \"..... a the, is not., the to to,, the., to, the,, the, to., to to to to to but to and.. to in. the to,...,\n"
     ]
    }
   ],
   "source": [
    "reference_text = testset[3]['text']\n",
    "prompt = \" \".join(reference_text.split(\" \")[:5])\n",
    "\n",
    "print(\"Unsteered: \\n\")\n",
    "print(model.llm_generate(prompt))\n",
    "print(\"Steered: \\n\")\n",
    "print(model(model.bert_tokenize(reference_text),prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431967-d513-4c38-a122-00a5e8ed282d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Small Data",
   "language": "python",
   "name": "small-data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
